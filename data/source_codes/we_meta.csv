,cell_type,cell_title,role,codes
1,md,,description,"## 전체 로직 설명

* STEP 1: not_null_list 테이블 업데이트

  * 앨범 메타 데이터 입력 시트에서 새로 입력된 데이터가 있다면 가져와 임시 테이블에 저장한다(inserted).

  * PROD 구글 시트에서 최신화된 기입력 데이터를 전부 가져온다(pre_exist).

  * 두 테이블을 nvl(inserted, pre_exist)해 데이터 최신화를 한 후 we_meta.ws_album_not_null_list에 전체 업데이트해 저장한다. 이 때 최신화된 수기 입력 데이터가 저장된다.

* STEP 2: ws_album 구성

  * weverseshop.goods_option_group.logistics_category = 'ALBUM'인 모든 상품들의 리스트 만들어 앨범 관련 상품 기본 정보와 함께 임시 테이블에 저장한다(album_list).

  * we_meta.ws_album_not_null_list에서 새로운 앨범 이름이 생겼다면 album_id를 새로 발급해 (we_art_id, 앨범 명, album_id)로 we_meta.ws_album_id에 저장한다.

  * we_meta.ws_album_not_null_list, album_list, we_meta.ws_album_id, we_mart.we_artist를 통합해 we_mart.ws_album의 금일자 part_date를 구성하고 dataflow로 append한다.

* STEP 3: 구성 완료 앨범 PROD 구글 시트에 쏘기


  * we_mart.ws_album에서 max_part_date를 찾은 후 전체 내용을 PROD 구글 시트에 옮겨 데이터를 최신화한다.

* STEP 4: null_list 테이블 업데이트

  * album_list와 we_meta.ws_album_not_null_list를 조인해 we_meta.ws_album_not_null_list에는 없는 sale_id들을 골라낸다.

  * 입력이 안된 앨범 상품들이므로 앨범 메타 데이터 입력 시트의 **[입력 대기] 미등록 앨범**에 쏴준다.

  * **[입력 대기] 미등록 앨범** 리스트에 있는 앨범 상품들 중 운영 데이터에서 지워진 상품들은 리스트에서 제거해주며 완료한다.

  * 대기 앨범에 기존 입력됐던 앨범명 중 고를 수 있게 설정한다. 대기 앨범이 기존 입력분이 없는 신규 앨범 시 경고창을 무시하고 새 앨범명을 입력한다."
2,md,,basic_info,"#### Basic Info
* 위버스샵 판매 앨범 메타 정보
* Mart Primary 
* HOURLY OVERWRITE
* WIKI : [LINK](https://bighitcorp.atlassian.net/wiki/spaces/OD/pages/3447325408/we+meta.ws+album) 
* 기타 : 위버스 샵에서 판매 중인 앨범들의 메타 테이블, 구글 시트로 입력 받은 수기 입력 정보와 어드민 정보 취합 후 구성

###### history
|Date |Contributor|Comment|
|:-------|:-------|:---------------------|
|2022-09-19 |송재영|마트생성|
|2023-01-18 |송재영|위버스 앨범 여부 => 위버스 앨범 포함 여부로 변경|
|2023-04-03 |송재영|시트에 album_id 보여주는 용도로 추가, 운영 데이터 변경으로 인해 비대상 상품이 되는 경우 제거 로직 추가|
|2023-07-18 |송재영|album_scm_option_type 필드 추가|
|2024-02-20 |송재영|ws_album_id에 we_art_id 필드 추가(한 앨범에 여러 아티스트가 속해 있을 경우엔 null, e.g., NCT 공통 앨범)|
|2024-02-20 |송재영|앨범 명이 같은데 아티스트는 다른 경우 앨범 ID 나누는 로직 적용(ws_album, ws_album_id)|
|2024-07-23 |송재영|앨범 필수 입력 필드 추가(앨범 유형 순번;album_content_seq), ws_album_latest 뷰 생성 쿼리 추가|

###### Source Tables
* we_meta.ws_album_not_null_list
* we_meta.ws_album_id
* we_mart.we_artist
* weverseshop.goods
* weverseshop.goods_stock
* weverseshop.stock
* weverseshop.goods_option
* weverseshop.goods_option_group
* weverseshop.sale
* weverseshop.goods_goods_category
* weverseshop.goods_category
* weverseshop.sale_stock
* weverseshop.goods_translation"
3,run,,etc, /Repos/databricks-prod/databricks/src/data_platform/dataflow/dataflow
4,py,,setting,"import datetime
from pyspark.sql import functions as f
from pyspark.sql.types import *
from pyspark.sql.functions import col
import warnings
warnings.filterwarnings(""ignore"")

try:
  import importlib
  import gspread
except:
  !pip install gspread==5.12.4
  import gspread

if gspread.__version__ == '6.0.0':
  !pip install --force-reinstall -v ""gspread==5.12.4""
  try:
    importlib.reload(gspread)
  except Exception:
    pass 

print(gspread.__version__) # 5.12.4

try:
  import gspread_formatting as gf
  from oauth2client.service_account import ServiceAccountCredentials
  from googleapiclient import discovery
except:
  !pip install --upgrade oauth2client
  !pip install google-api-python-client
  !pip install gspread-formatting
  
  import gspread_formatting as gf
  from oauth2client.service_account import ServiceAccountCredentials
  from googleapiclient import discovery

import pandas as pd
import re

# authorization
scope = [
  'https://www.googleapis.com/auth/drive'
]
json_file_name = '/dbfs/FileStore/tables/elite_elevator_388506_d4350aba3f9b.json'

credentials = ServiceAccountCredentials.from_json_keyfile_name(json_file_name, scope)
gc = gspread.authorize(credentials)

spark.sql(""""""use catalog 'wev_prod'"""""")"
5,,,etc," %md
 ##STEP 1. not_null_list 업데이트"
6,py,,code,"# 로그 설계 문서에서 페이지 별 정의서 시트, 데이터 리스트 가져오기
base_url = ""https://docs.google.com/spreadsheets/d/1wcHtb6S71NV_-IV5y5y6P3iRF0Dqo6FO2DgJ87JL-5A/edit#gid=1552492115"""
7,py,,code,"# column 명 정의 및 한글과 맵핑
cols = dict()

# 입력 전 상품 등록 후 자동 생성 영역(1~5)
cols['아티스트 명'] = 'we_art_name'
cols['위버스 샵 명'] = 'shop'
cols['SALE_ID'] = 'sale_id'
cols['상품 명'] = 'goods_name'
cols['상품 앨범 수량'] = 'goods_album_qty'

# 앨범명/아티스트 영역(6~9)
cols['앨범 명'] = 'album_name'
cols['앨범 별칭 리스트'] = 'album_alias_list'
cols['앨범 참여 아티스트 타입'] = 'album_performer_type'
cols['앨범 참여 아티스트 활동 명'] = 'album_performer_name'

# 앨범 유형/옵션 영역(10~13)
cols['앨범 유형'] = 'album_content_type'
cols['앨범 유형 순번'] = 'album_content_seq'
cols['앨범 옵션 명'] = 'album_option_type'
cols['앨범 수량 유형'] = 'album_qty_type'
cols['앨범 SCM 옵션 구성'] = 'album_scm_option_type'

# 기타 정보/날짜 데이터 영역(14~21)
cols['실물 앨범 유형'] = 'album_physical_type'
cols['위버스 독점 여부'] = 'is_wev_exclusive'
cols['위버스 앨범 포함 여부'] = 'is_wa_included'
cols['타겟 국가/지역'] = 'target_ctry'
cols['앨범 발매일'] = 'album_release_date'

## cols['상품 판매 시작일'] = 'sale_start_date' -- 구하기 힘듦
cols['사용 가능 여부'] = 'is_enable'
cols['입력자'] = 'upd_by'
cols['입력일'] = 'upd_date'

# 자동 계산 영역(22~23)
cols['초동 집계 마감일'] = 'album_cumulative_date'
cols['프로젝트 명'] = 'project_name'
cols['앨범 ID'] = 'album_id'

cols_eng = [i for i in cols.values()]
cols_kor = [i for i in cols.keys()]
# print(cols)

cols_idx = dict()
for i, key in enumerate(cols_kor):
  cols_idx[i] = key
  
cols_idx_ = dict()
for key, value in cols_idx.items():
  cols_idx_[value] = key"
8,py,,code,"def auto_generated_fields(data_row, cols_idx, cols_idx_, cols):
  """"""
  자동 생성 필드 
  """"""

  rel_date = datetime.datetime.strptime(data_row[cols_idx_['앨범 발매일']], '%Y-%m-%d')
  print(rel_date)

  # 초동 마감 일자 계산(발매 일 + 6일)
  album_cumulative_date = rel_date + datetime.timedelta(days = 6)
  album_cumulative_date = album_cumulative_date.strftime('%Y-%m-%d')

  # 프로젝트 명(임시)
  project_name = rel_date.strftime(""[%y'%m] "") + data_row[cols_idx_['앨범 명']]

  data_row = data_row + [album_cumulative_date, project_name, None]  
  return data_row"
9,py,,code,"base_doc = gc.open_by_url(base_url)

# 기입 완료한 앨범 상품들을 담을 데이터 프레임 생성
df = pd.DataFrame(columns = cols_eng)

# 시트 선택하기
ws = base_doc.worksheet('[입력 대기] 미등록 앨범')

# 행들 중 상단 데이터가 아닌 부분
non_data_rows = 4

num_of_data = len(ws.col_values(1)) - non_data_rows
inserted = None
if num_of_data > 0: #데이터가 하나라도 있으면 
  range_ws = ('A' + str(non_data_rows + 1), chr(len(ws.row_values(non_data_rows)) + ord('A') - 1) + str(len(ws.col_values(cols_idx_['앨범 명'])))) #범위: [A][데이터 위로 공지사항, 헤더 행 이후 행의 숫자] : [헤더가 존재하는 열][앨범 명 마지막 행 숫자]
  # 데이터 가져오기
  data = ws.get(range_ws[0] + ':' + range_ws[1])

  # 데이터 프레임에 데이터 넣기
  to_delete = []
  for i in range(len(data)):
    if len(data[i]) == 23: # 홀드 걸어놓는 값이 있을 경우
      insert_values = data[i][:-1] #홀드 행 다른 행은 전부 가져오기
      hold = data[i][-1] # 홀드 값은 따로 저장
    else: # 없을 경우 그냥 다 가져오기, hold는 None
      insert_values = data[i]
      hold = None
    
    # 필수 입력 항목 기입 여부 체크
    indices = set(j for j in range(len(insert_values)) if insert_values[j] != '')
    if {\
       cols_idx_['앨범 명']\
     , cols_idx_['앨범 참여 아티스트 타입']\
     , cols_idx_['앨범 유형']\
     , cols_idx_['앨범 유형 순번']\
     , cols_idx_['앨범 옵션 명']\
     , cols_idx_['앨범 수량 유형']\
     , cols_idx_['실물 앨범 유형']\
     , cols_idx_['타겟 국가/지역']\
     , cols_idx_['앨범 발매일']\
     , cols_idx_['입력자']\
     , cols_idx_['입력일']\
     }.issubset(indices):
      
      if hold != 'YES':
        to_delete.append(i + 5)
             
      row = pd.Series(auto_generated_fields(insert_values, cols_idx, cols_idx_, cols), index = cols_eng)
      df = df.append(row, ignore_index = True)
      
  if df.empty == False:
    inserted = spark.createDataFrame(df)
  
  # 입력 정상 완료된 row는 지우기
  for i, d_index in enumerate(to_delete):
    ws.delete_rows(d_index - i)


else: # 입력 대상이 아무 것도 없는 경우
  inserted = None

display(inserted)"
10,py,,code,"#dataframe not_null_values
df = pd.DataFrame(columns = cols_eng)

# 시트 선택하기
ws = base_doc.worksheet('[입력 완료] 앨범 메타 정보')

# 데이터 가져오기
last_col_letter = 'X'
columns = ws.get('A4:' + str(len(ws.row_values(4))))[0][:ord(last_col_letter) - 64]
values = ws.get('A5:' + last_col_letter + str(len(ws.col_values(ord('A') - 64))))

l = list()
for vs in values:
  nl = dict()
  for k, v in zip(columns, vs): 
    if k in cols_kor:
      # print(cols[k], v)
      nl[cols[k]] = v
  l.append(nl)

# 데이터 프레임에 데이터 넣기
for i in range(len(values)):
  try:
    row = pd.Series(l[i], index = cols_eng)
  except:
    row = pd.Series(l[i] + [''], index = cols_eng)
  df = df.append(row, ignore_index = True)

pre_exist = spark.createDataFrame(df)
display(df)"
11,py,,code,"# 중복이 발생하면 최근 입력 데이터로 업데이트
duplicate_q = f""""""
select 
nvl(a.we_art_name, b.we_art_name) as we_art_name
, nvl(a.shop, b.shop) as shop
, nvl(a.sale_id, b.sale_id) as sale_id
, nvl(a.goods_name, b.goods_name) as goods_name
, nvl(a.goods_album_qty, b.goods_album_qty) as goods_album_qty
, trim(nvl(a.album_name, b.album_name)) as album_name
, nvl(a.album_alias_list, b.album_alias_list) as album_alias_list
, nvl(a.album_performer_type, b.album_performer_type) as album_performer_type
, nvl(a.album_performer_name, b.album_performer_name) as album_performer_name
, nvl(a.album_content_type, b.album_content_type) as album_content_type
, nvl(a.album_content_seq, b.album_content_seq) as album_content_seq
, nvl(a.album_option_type, b.album_option_type) as album_option_type
, nvl(a.album_qty_type, b.album_qty_type) as album_qty_type
, nvl(a.album_scm_option_type, b.album_scm_option_type) as album_scm_option_type
, nvl(a.album_physical_type, b.album_physical_type) as album_physical_type
, nvl(a.is_wev_exclusive, b.is_wev_exclusive) as is_wev_exclusive
, nvl(a.target_ctry, b.target_ctry) as target_ctry
, nvl(a.album_release_date, b.album_release_date) as album_release_date
, null as sale_start_date
, nvl(a.is_enable, b.is_enable) as is_enable
, nvl(a.is_wa_included, b.is_wa_included) as is_wa_included
, nvl(a.upd_by, b.upd_by) as upd_by
, nvl(a.upd_date, b.upd_date) as upd_date
, nvl(a.album_release_date, b.album_release_date) + interval 6 days as album_cumulative_date
, nvl(a.project_name, b.project_name) as project_name
, b.album_id
from inserted as a
full outer join pre_exist as b
on a.sale_id = b.sale_id
"""""""
12,py,,code,"if inserted != None:
  inserted.createOrReplaceTempView('inserted')
  pre_exist.createOrReplaceTempView('pre_exist')
  fin_df = spark.sql(duplicate_q)
  no_change = False
else:
  fin_df = pre_exist
  no_change = True
  
if no_change == False:
  fin_df = fin_df.withColumn(""sale_id"", col(""sale_id"").cast(IntegerType()))\
    .withColumn(""album_content_seq"", col(""album_content_seq"").cast(IntegerType()))\
    .withColumn(""album_release_date"", col(""album_release_date"").cast(DateType()))\
    .withColumn(""album_cumulative_date"", col(""album_cumulative_date"").cast(DateType()))\
    .withColumn(""sale_start_date"", col(""sale_start_date"").cast(DateType()))\
    .withColumn(""album_performer_name"", f.when(fin_df.album_performer_name.isNull(), 'NULL').otherwise(fin_df.album_performer_name))\
    .withColumn(""album_alias_list"", f.when(fin_df.album_alias_list.isNull(), f.array('album_name')).otherwise(f.split(fin_df.album_alias_list, r"", "")))
#     .withColumn(""performer_name"", F.when(fin_df.performer_name.isNull(), 'NULL').otherwise(fin_df.performer_name))
    #.withColumn('album_physical_type', lit('NULL'))
  fin_df.createOrReplaceTempView('tmp_album_not_null_list')

  spark.sql(""CREATE OR REPLACE TABLE we_meta.ws_album_not_null_list AS SELECT * FROM tmp_album_not_null_list"")"
13,md,,description,##STEP 2. we_meta.ws_album 구성
14,py,,setting,"key = dbutils.widgets.get(""target_date"")
run_mode = dbutils.widgets.get(""run_mode"")

slack_token = dbutils.secrets.get(scope=""slack"", key=""slack-token"")
channel = dbutils.secrets.get(scope=""slack"", key=""analytics-channel"") #""#data-analytics-alert-test""

noti = {
  'channel' : channel,
  'token' : slack_token
}
table = {
  'database' : 'we_meta',
  'table_name' : 'ws_album', 
  'service' : 'weverseshop', #default (None)
  'partition' : ['part_date']
}

option = {
  'date' : key,
  'format': 'delta', #default (delta)
  'mode' : 'append', #default (append)
  'period' : 'daily', #default (daily)
  'noti' : True, #default (True)
}

spark.sql('set time zone ""Asia/Seoul""')"
15,md,,just_heading,#### Main Query
16,py,,code,"album_list = spark.sql(""""""
select distinct GDS.shop, GDS_name.goods_name, SLE.sale_id, ART.we_art_name, GOG.album_quantity as album_qty, GDS.goods_id, GDO.goods_option_id, GDO.goods_option_code, GDO.sap_code, GDO.name as goods_option_name, STK.stock_id, SLT.sale_stock_id
, SLE.shop_tags
from weverseshop.goods as GDS
left join weverseshop.goods_stock as GDT on GDS.goods_id = GDT.goods_id
left join weverseshop.stock as STK on GDT.stock_id = STK.stock_id
left join weverseshop.goods_option as GDO on STK.goods_option_id = GDO.goods_option_id
left join weverseshop.goods_option_group as GOG on GDO.goods_option_group_id = GOG.goods_option_group_id
left join weverseshop.sale as SLE on GDS.goods_id = SLE.goods_id 
left join weverseshop.goods_goods_category as GGC on GDS.goods_id = GGC.goods_id
left join weverseshop.goods_category as GCU on GGC.goods_category_id = GCU.goods_category_id
left join we_mart.we_artist as ART on  GCU.label_artist_id = ART.ws_art_id
left join weverseshop.sale_stock as SLT on SLE.sale_id = SLT.sale_id and GDO.goods_option_id = SLT.goods_option_id
left join
(
  select 
  goods_id
  , case
      when language = 'ko' then 'GL'
      when language = 'en' then 'US'
      when language = 'ja' then 'JP'
    end as shop
  , name as goods_name
  from weverseshop.goods_translation
) as GDS_name on GDS.goods_id = GDS_name.goods_id and GDS.shop = GDS_name.shop
where GOG.logistics_category in ('ALBUM')
and ART.we_art_id != 44 -- 이진혁 종료된 커뮤니티 제외
and SLE.sale_id is not null
and SLE.sale_id not in (15079, 18060) -- TEST 건들
and SLE.sale_id not in ( -- 앨범 메타에서 제외돼야 할 상품 리스트, ws_album_exc 테이블에서 수기로 인입됨
  select distinct sale_id
  from wev_prod.we_meta.ws_album_exc
) 
""""""
)
album_list.createOrReplaceTempView(""album_list"")
print('no change?: ', no_change)"
17,py,,code,"# 앨범 ID 불러오기
# 만약 새로운 이름의 앨범이 있다면 새로운 ID 부여
max_album_id = spark.sql('select max(album_id) from we_meta.ws_album_id').collect()[0][0]

spark.sql(f""""""
with albums(
  select *, case when dense_rank() over (partition by album_name order by sale_id) = 1 then 1 else 0 end as rank
  from (
    select a.we_art_name, sale_id, a.album_name, b.album_id
    from wev_prod.we_meta.ws_album_not_null_list as a
    left join wev_prod.we_meta.ws_album_id as b
    on a.album_name = b.album_name and a.album_id = b.album_id
    where a.album_id is null
  )
  order by sale_id desc
)
select * from we_meta.ws_album_id
union all
select we_art_id, album_name, rank() over (order by max_sale_id) + {max_album_id} as album_id
from (
  select insert.*
  from (
    select case when arts != 1 then null else c.we_art_id end as we_art_id
    , a.album_name, max_sale_id
    from albums as a
    left join (
      select album_name, count(distinct we_art_name) as arts, max(sale_id) as max_sale_id
      from albums
      group by 1
    ) as b
    on a.album_name = b.album_name
    left join wev_prod.we_mart.we_artist as c
    on a.we_art_name = c.we_art_name
    where a.rank = 1
  ) as insert
  left join wev_prod.we_meta.ws_album_id as ex
  on case when ex.we_art_id is not null then insert.we_art_id = ex.we_art_id and insert.album_name = ex.album_name else insert.album_name = ex.album_name end
  where ex.album_id is null
)
order by album_id desc
""""""
).write.mode('overwrite').saveAsTable(""wev_prod.we_meta.ws_album_id"")
display(spark.read.table('wev_prod.we_meta.ws_album_id'))"
18,py,,code,"from pyspark.sql.types import MapType, StringType, IntegerType
from pyspark.sql.functions import from_json

album_df = spark.sql(""""""
select distinct
-- ARIST_META
  WE_ART.ws_label_id, WE_ART.ws_label_name
, WE_ART.we_art_id, WE_ART.we_art_name
, WE_ART.ws_art_id, WE_ART.ws_art_name as artist_name

-- ALBUM_META 7
, MAN_INS.album_performer_type
, MAN_INS.album_performer_name
, MAN_INS.project_name
, MAN_INS.album_content_type
, MAN_INS.album_content_seq
, ID.album_id
, MAN_INS.album_name
, MAN_INS.album_alias_list
, MAN_INS.album_option_type
, MAN_INS.album_physical_type
, MAN_INS.album_qty_type
, MAN_INS.album_scm_option_type
, MAN_INS.album_release_date
, MAN_INS.album_cumulative_date

-- SALES_INFO 21~
, ALBUM_ADMIN.album_qty
, ALBUM_ADMIN.shop
, MAN_INS.target_ctry
, ALBUM_ADMIN.sale_id
, collect_set(ALBUM_ADMIN.stock_id) as stock_ids
, collect_set(ALBUM_ADMIN.sale_stock_id) as sale_stock_ids
, case when MAN_INS.is_wev_exclusive = 'YES' then tinyint(1) else tinyint(0) end as is_wev_exclusive
, MAN_INS.sale_start_date
 
-- GOODS_INFO 29
, cast(ALBUM_ADMIN.goods_id as int)
, ALBUM_ADMIN.goods_name
, collect_set(ALBUM_ADMIN.sap_code) as sap_codes
, collect_set(struct(ALBUM_ADMIN.goods_option_id, ALBUM_ADMIN.goods_option_code, ALBUM_ADMIN.goods_option_name)) as goods_option_values

-- ETC 33
, case when MAN_INS.is_wa_included = 'YES' or ALBUM_ADMIN.shop_tags rlike 'WEVERSE_ALBUM' then tinyint(1) else tinyint(0) end as is_wa_included
, case when MAN_INS.is_enable = 'YES' then tinyint(1) else tinyint(0) end as is_enable
, MAN_INS.upd_by
, cast(MAN_INS.upd_date as date)

, current_timestamp() as run_timestamp
, '{key}' as part_date

from we_meta.ws_album_not_null_list as MAN_INS --(manual insert)
left join we_mart.we_artist as WE_ART
on MAN_INS.we_art_name = WE_ART.we_art_name
left join album_list as ALBUM_ADMIN
on MAN_INS.sale_id = ALBUM_ADMIN.sale_id
left join we_meta.ws_album_id as ID
on MAN_INS.album_name = ID.album_name and case when ID.we_art_id is not null then WE_ART.we_art_id = ID.we_art_id else 1=1 end
group by 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 33, 34, 35, 36
order by we_art_id, album_name, sale_id
"""""".format(key = key)
)

album_df = album_df.withColumn(""album_scm_option_type"",\
    from_json(album_df.album_scm_option_type, MapType(StringType(), IntegerType())))"
19,md,,just_heading,#### Run
20,py,,code,"b = Dataflow(run_mode=run_mode, notifier=noti)

if no_change == False:
  b.run(dataframe=album_df, table_info=table, option=option, buckets=['databricks'])
else:
  tmp_df = spark.sql(""select * from we_meta.ws_album where part_date = '{key}'"".format(key = key))
  if len(tmp_df.collect()) == 0: # key 일자가 비었거나 없다면 전 일자 꺼를 그대로 가져오기
    key_delta = datetime.datetime.strptime(key, '%Y-%m-%d') - datetime.timedelta(days = 1)
    key_delta = key_delta.strftime('%Y-%m-%d')
    tmp_df = spark.sql(""""""
      select ws_label_id
        , ws_label_name
        , we_art_id
        , we_art_name
        , ws_art_id
        , artist_name
        , album_performer_type
        , album_performer_name
        , project_name
        , album_content_type
        , album_content_seq
        , album_id
        , album_name
        , album_alias_list
        , album_option_type
        , album_physical_type
        , album_qty_type
        , album_scm_option_type
        , album_release_date
        , album_cumulative_date
        , album_qty
        , shop
        , target_ctry
        , sale_id
        , stock_ids
        , sale_stock_ids
        , is_wev_exclusive
        , sale_start_date
        , goods_id
        , goods_name
        , sap_codes
        , goods_option_values
        , is_wa_included
        , is_enable
        , upd_by
        , upd_date
        , current_timestamp() as run_timestamp
        , '{key}' as part_date
      from we_meta.ws_album where part_date = '{key_delta}'
      """""".format(key_delta = key_delta, key = key))
    
  b.run(dataframe=tmp_df, table_info=table, option=option, buckets=['databricks'])
Dataflow().refresh_tableau_datasource_by_name(""META_WS_ALBUM_EXTRACT"")"
21,md,,just_heading,#### Appendix
22,md,,just_heading,###### Create Table
23,py,ws_album CREATE,code,"q = """"""
create or replace table we_meta.ws_album
(
-- 아티스트 메타 영역
  ws_label_id	bigint	comment	""아티스트 소속 레이블 ID""
, ws_label_name	string	comment	""아티스트 소속 레이블명""
, we_art_id	int	comment	""we_art_id""
, we_art_name	string	comment	""we_art_name""
, ws_art_id	bigint	comment	""위버스 샵 레이블/아티스트 ID""
, artist_name	string	comment	""위버스 샵 기준 아티스트명""

-- 앨범 메타 영역
, album_performer_type	string	comment	""앨범 참여 아티스트 구성 유형 (그룹, 유닛, 솔로)""
, album_performer_name	string	comment	""앨범 참여 아티스트 명(nvl(별칭, 그룹명))""
, project_name	string	comment	""프로젝트 명""
, album_content_type	string	comment	""앨범 유형 (싱글, 정규, 미니.. )""
, album_content_seq	int	comment	""앨범 유형 별 순번(ex, BTS 정규 3집 => 3)""
, album_id	int	comment	""앨범 고유 번호""
, album_name	string	comment	""정식 앨범명""
, album_alias_list	array<string>	comment	""앨범명이 온전한 영어가 아닐 경우 대신 사용 가능한 앨범 별칭 어레이""
, album_option_type	string	comment	""앨범 패키지 및 형태 구분 (디럭스, 일반, 위버스앨범, 스페셜, 콤팩트, KiT..)""
, album_physical_type string comment ""앨범 물리 형태 구분(실물 앨범 유형)""
, album_qty_type	string	comment	""앨범 판매 단위 종류 (랜덤, 세트, 옵션 등)""
, album_scm_option_type	map<string,int>	comment	""SCM팀 사용 앨범 판매 단위 종류""
, album_release_date	date	comment	""앨범 발매일""
, album_cumulative_date	date 	comment	""앨범 초동 판매 마감일""

-- 판매 관련
, album_qty	int	comment	""판매 단위당 앨범 수량""
, shop	string	comment	""판매 숍""
, target_ctry	string	comment	""타깃국가 (글로벌, 일본)""
, sale_id	bigint	comment	""판매 ID""
, stock_ids	array<bigint>	comment	""입고 ID list""
, sale_stock_ids	array<bigint>	comment	""판매 상품 옵션 ID list""
, is_wev_exclusive	tinyint		comment '위버스 독점 판매 여부'
, sale_start_date	date	comment	""상품 판매 시작일""


-- 상품 메타 영역
, goods_id	int	comment	""상품 ID""
, goods_name	string	comment	""상품명""
, sap_codes		array<string>	comment ""SAP 코드 리스트""
, goods_option_values	array<struct<goods_option_id:long, goods_option_code:string, goods_option_name:string>>	comment	""상품 옵션 ID, CODE, 명""

-- 기타 영역
, is_wa_included	int	comment	""위버스 앨범 여부""
, is_enable	int	comment ""상품 사용가능 여부""
, upd_by	string	comment ""마지막 입력자""
, upd_date	date	comment ""마지막 입력일""
, run_timestamp timestamp comment ""데이터 입력 일시""
, part_date string comment ""파티션 일자""
) 
partitioned by (part_date)
comment ""위버스 샵 판매 앨범 메타 정보""
""""""
# spark.sql(q)"
24,py,ws_album_exc CREATE,code,"## 앨범 메타 데이터에 제외돼어야 하는 상품 리스트, 하드코딩으로 제외 상품 입력하기
q2 = """"""
create or replace table wev_prod.we_meta.ws_album_exc
(
  album_id int comment ""앨범 고유 번호(ws_album)""
, album_name	string	comment	""정식 앨범명""
, sale_id	bigint	comment	""판매 ID""
, requested_by  string  comment ""제외 요청자""
, requested_at  string  comment ""제외 요청일""
)
comment ""위버스 샵 판매 앨범 메타에서 제외돼야 할 앨범 상품 리스트""
""""""
# spark.sql(q2)"
25,py,,code,"## ws_album_exc에 제외 상품 추가하는 예시, 로컬 노트북에서 실행해도 됨.
insert_q = """"""
insert into wev_prod.we_meta.ws_album_exc
select 468,	""SPILL THE FEELS"", 29451, ""이한나"",  ""2024-09-11""
""""""
# spark.sql(insert_q) "
26,md,,just_heading,###### ws_album의 최신 상태 뷰 생성: ws_album_latest
27,py,,code,"latest_view_q = """"""
create or replace view wev_prod.we_meta.ws_album_latest as
select *
from wev_prod.we_meta.ws_album, (select max(part_date) as max_date from wev_prod.we_meta.ws_album) as b
where part_date = b.max_date
""""""

# spark.sql(latest_view_q)"
28,md,,just_heading,###### ws_album 앨범별 종합 정보 뷰 생성: ws_album_info
29,py,,code,"info_view_q = """"""
create or replace view wev_prod.we_meta.ws_album_info as
with latest as (
    select *
    from wev_prod.we_meta.ws_album as a,
    (
        select max(part_date) as latest
        from wev_prod.we_meta.ws_album
    ) as b
  where a.part_date = b.latest
)
select
    ws_label_id
  , ws_label_name
  , we_art_id
  , we_art_name
  , ws_art_id
  , artist_name
  , album_id
  , album_name
  , project_name
  , album_performer_type
  , album_performer_name
  , album_content_type
  , album_content_seq
  , album_release_date
  , album_cumulative_date
  , target_ctry
  , max(is_wa_included) as is_wa_goods_exist
  , collect_set(shop) as sale_shops
from latest
group by
    ws_label_id
  , ws_label_name
  , we_art_id
  , we_art_name
  , ws_art_id
  , artist_name
  , album_id
  , album_name
  , project_name
  , album_performer_type
  , album_performer_name
  , album_content_type
  , album_content_seq
  , album_release_date
  , album_cumulative_date
  , target_ctry
order by ws_label_id
  , ws_label_name
  , we_art_id
  , project_name
""""""

# spark.sql(info_view_q)"
30,md,,just_heading,## STEP 3. 구성 완료 앨범 PROD 구글 시트에 쏘기
31,py,상품별 앨범 메타 정보 시트,code,"# we_meta에서 가져온 후 prod 시트에 덮어쓰기
working_ws = base_doc.worksheet('[입력 완료] 앨범 메타 정보')
max_part_date = spark.sql('select max(part_date) from we_meta.ws_album').collect()[0][0]

df = spark.sql(""""""
select 
  project_name
, we_art_name
, shop
, sale_id
, goods_name
, album_qty
, album_id
, album_name
, array_join(album_alias_list, "", "") as album_alias_list
, album_performer_type
, album_performer_name
, album_content_type
, album_content_seq
, album_option_type
, album_qty_type
, album_scm_option_type
, album_physical_type
, case when is_wev_exclusive = 1 then 'YES' else 'NO' end as is_wev_exclusive
, case when is_wa_included = 1 then 'YES' else 'NO' end as is_wa_included
, target_ctry
, string(date_format(album_release_date, 'yyyy-MM-dd')) as album_release_date
, case when is_enable = 1 then 'YES' else 'NO' end as is_enable
, upd_by
, string(date_format(upd_date, 'yyyy-MM-dd')) as upd_date
from we_meta.ws_album
where part_date = '{max_part_date}'
order by we_art_name, album_name, sale_id
"""""".format(max_part_date = max_part_date)
).toPandas().fillna('')

df['album_scm_option_type'] = df['album_scm_option_type'].apply(lambda x: str(x))

data_list = df.values.tolist()
last_col_letter = 'X'

working_ws.update(values=data_list, range_name='A5:' + last_col_letter + str(len(data_list) + 4))
working_ws.format(
  'A5:' + last_col_letter + str(len(data_list) + 4),
  {
    ""horizontalAlignment"": ""LEFT"",
    ""textFormat"": {
      ""fontSize"": 10,
      ""bold"": False
    },
    ""borders"":{
      ""top"": {
        ""style"": ""SOLID""
      },
      ""bottom"": {
        ""style"": ""SOLID""
      },
      ""left"": {
        ""style"": ""SOLID""
      },
      ""right"": {
        ""style"": ""SOLID""
      }
    }
  }
)

working_ws.update(values=[[
  ""프로젝트 명""
, ""아티스트 명""
, ""위버스 샵 명""
, ""SALE_ID""
, ""상품 명""
, ""상품 앨범 수량""
, ""앨범 ID""
, ""앨범 명""
, ""앨범 별칭 리스트""
, ""앨범 참여 아티스트 타입""
, ""앨범 참여 아티스트 활동 명""
, ""앨범 유형""
, ""앨범 유형 순번""
, ""앨범 옵션 명""
, ""앨범 수량 유형""
, ""앨범 SCM 옵션 구성""
, ""실물 앨범 유형""
, ""위버스 독점 여부""
, ""위버스 앨범 포함 여부""
, ""타겟 국가/지역""
, ""앨범 발매일""
, ""사용 가능 여부""
, ""입력자""
, ""입력일""]], range_name=""A4:"" + last_col_letter + ""4"")

working_ws.set_basic_filter(""A4:{letter}{rows}"".format(letter = last_col_letter, rows = len(working_ws.col_values(1))))"
32,py,아티스트별 발매 앨범 정보 시트,code,"working_ws = base_doc.worksheet('아티스트별 발매 앨범 정보')
df_info = spark.sql(""""""
  select
  project_name
  , we_art_name
  , we_art_id
  , album_id
  , album_name
  , album_performer_type
  , album_performer_name
  , concat('[', target_ctry, '] ', album_content_type, "" "", case when album_content_seq != -1 then concat(string(album_content_seq), '집') else '' end) as album_info
  , target_ctry
  , album_content_type
  , album_content_seq
  , case when is_wa_goods_exist = 1 then 'YES' else 'NO' end as is_wa_included
  , string(date_format(album_release_date, 'yyyy-MM-dd')) as album_release_date
  , string(date_format(album_cumulative_date, 'yyyy-MM-dd')) as album_cumulative_date
  from wev_prod.we_meta.ws_album_info
  order by we_art_id, project_name
""""""
).toPandas().fillna('')

data_list = df_info.values.tolist()
last_col_letter = 'N'

working_ws.update(values=data_list, range_name='A2:' + last_col_letter + str(len(data_list) + 1))
formats = []
formats.append(
  {
    ""range"" : 'A2:' + last_col_letter + str(len(data_list) + 1),
    ""format"" : {
      ""horizontalAlignment"": ""CENTER"",
      ""textFormat"": {
        ""fontSize"": 10,
        ""bold"": False
      },
      ""borders"":{
        ""top"": {
          ""style"": ""SOLID""
        },
        ""bottom"": {
          ""style"": ""SOLID""
        },
        ""left"": {
          ""style"": ""SOLID""
        },
        ""right"": {
          ""style"": ""SOLID""
        }
      }
    }
  }
)

formats.append(
  {
    ""range"" : 'A2:A' + str(len(data_list) + 1), 
    ""format"" : {
      ""horizontalAlignment"": ""LEFT"",
      ""textFormat"": {""fontSize"": 10,""bold"" : True},
    }
  }
)
formats.append(
  {
    ""range"" : 'E2:E' + str(len(data_list) + 1), 
    ""format"" : {
      ""horizontalAlignment"": ""LEFT"",
      ""textFormat"": {""fontSize"": 10,""bold"" : False},
    }
  }
)
formats.append(
  {
    ""range"" : 'H2:H' + str(len(data_list) + 1), 
    ""format"" : {
      ""horizontalAlignment"": ""LEFT"",
      ""textFormat"": {""fontSize"": 10,""bold"" : False},
    }
  }
)
working_ws.batch_format(formats)
working_ws.set_basic_filter(""A1:{letter}{rows}"".format(letter = last_col_letter, rows = len(working_ws.col_values(1))))"
33,md,,just_heading,## STEP 4: null_list 업데이트
34,py,,code,"q =f""""""
create or replace table we_meta.ws_album_null_list
select distinct a.we_art_name, a.shop, a.sale_id, a.goods_name, a.album_qty
from album_list as a
left join we_meta.ws_album_not_null_list as b
on a.sale_id = b.sale_id
where 1=1
and a.sale_id is not null
and b.sale_id is null
""""""
spark.sql(q)"
35,py,,code,"# 로그 설계 문서에서 페이지 별 정의서 시트, 데이터 리스트 가져오기
base_doc = gc.open_by_url(base_url)
ws = base_doc.worksheet('[입력 대기] 미등록 앨범')

df = spark.sql(""""""
select * 
from we_meta.ws_album_null_list
where 1=1
and album_qty is not null
order by sale_id asc
"""""").toPandas()
data_list = df.values.tolist()"
36,py,,code,"# 이미 리스트에 있는 상품은 제외
exists = list()
for a,s in zip(ws.col_values(1)[4:], ws.col_values(3)[4:]):
  exists.append((a,int(s)))

removes = 0
for idx in range(len(data_list)):
  if (data_list[idx - removes][0], data_list[idx - removes][2]) in exists:
    data_list.pop(idx - removes)
    removes += 1

# 이미 리스트에 있는 상품 리스트 밑으로 추가
rows = len(ws.col_values(1)) - 1
ws.update(values=data_list, range_name='A' + str(2 + rows) + ':E10000')"
37,py,,code,"# 있으면 안되는 행 제거(보통 운영 데이터 변경으로 인해 지워진 아이템들, sale_id 기준)
item_list = []
for r in album_list.select(""we_art_name"", ""sale_id"").collect():
    item_list.append(tuple(r))

removes = 0
for i, r in enumerate(exists):
    if r not in item_list:
        ws.delete_rows(i + 5 - removes)
        removes += 1
print(""removed rows from the list: {}"".format(removes))
rows = len(ws.col_values(1)) - 1"
38,py,,code,"# 대기 앨범의 앨범 명 셀에 기존 입력된 앨범명 리스트에서 선택 가능하게 구성
def valid_rule(art_re_name, not_in_artist_list = False):
    if not_in_artist_list:
        target_artist = ""EMPTY_ARTIST""
    else:
        target_artist = art_re_name
    return gf.DataValidationRule(
        gf.BooleanCondition(""ONE_OF_LIST"", [i.value for i in ws.range(name = target_artist)]), showCustomUi=True
    )

artist_list = base_doc.worksheet('아티스트_앨범명').row_values(3)

print(gspread.__version__) # 5.12.4

# for i in range(5, rows + 2):
#     art_name = ws.get(""A"" + str(i))[0][0]
#     art_re = re.sub(r""[^a-zA-Z]"", """", art_name)
#     print(art_name, art_re)
#     gf.set_data_validation_for_cell_range(ws, range = ""F"" + str(i), rule = valid_rule(art_re_name = art_re, not_in_artist_list = art_name not in artist_list))

# 2024-01-29 gspread가 6.0이 되면서 gspread_formatting와 호환이 안됨, 버전 다운을 명시적으로 했으나 DAG에서 돌 땐 계속 충돌이 남(로컬에서 돌릴 땐 5.12.4 버전으로 다운그레이드 하면 잘 돌아감) 우선 해당 기능 off 하는 것으로 처리
"
